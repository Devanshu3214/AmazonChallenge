{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 3168,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00946969696969697,
      "grad_norm": 5.728010654449463,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 7.9319,
      "step": 10
    },
    {
      "epoch": 0.01893939393939394,
      "grad_norm": 5.016985893249512,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 7.9541,
      "step": 20
    },
    {
      "epoch": 0.028409090909090908,
      "grad_norm": 5.925593852996826,
      "learning_rate": 3e-06,
      "loss": 7.9303,
      "step": 30
    },
    {
      "epoch": 0.03787878787878788,
      "grad_norm": 5.642951011657715,
      "learning_rate": 4.000000000000001e-06,
      "loss": 7.9049,
      "step": 40
    },
    {
      "epoch": 0.04734848484848485,
      "grad_norm": 5.537333965301514,
      "learning_rate": 5e-06,
      "loss": 7.9111,
      "step": 50
    },
    {
      "epoch": 0.056818181818181816,
      "grad_norm": 5.332396030426025,
      "learning_rate": 6e-06,
      "loss": 7.8974,
      "step": 60
    },
    {
      "epoch": 0.06628787878787878,
      "grad_norm": 5.951898097991943,
      "learning_rate": 7.000000000000001e-06,
      "loss": 7.9177,
      "step": 70
    },
    {
      "epoch": 0.07575757575757576,
      "grad_norm": 5.404748916625977,
      "learning_rate": 8.000000000000001e-06,
      "loss": 7.8469,
      "step": 80
    },
    {
      "epoch": 0.08522727272727272,
      "grad_norm": 6.2264814376831055,
      "learning_rate": 9e-06,
      "loss": 7.7765,
      "step": 90
    },
    {
      "epoch": 0.0946969696969697,
      "grad_norm": 6.015793323516846,
      "learning_rate": 1e-05,
      "loss": 7.7381,
      "step": 100
    },
    {
      "epoch": 0.10416666666666667,
      "grad_norm": 6.015870571136475,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 7.7403,
      "step": 110
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 7.085954666137695,
      "learning_rate": 1.2e-05,
      "loss": 7.7171,
      "step": 120
    },
    {
      "epoch": 0.12310606060606061,
      "grad_norm": 6.007977485656738,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 7.6043,
      "step": 130
    },
    {
      "epoch": 0.13257575757575757,
      "grad_norm": 7.687938690185547,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 7.5705,
      "step": 140
    },
    {
      "epoch": 0.14204545454545456,
      "grad_norm": 7.374483585357666,
      "learning_rate": 1.5e-05,
      "loss": 7.6001,
      "step": 150
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 7.283661365509033,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 7.5465,
      "step": 160
    },
    {
      "epoch": 0.16098484848484848,
      "grad_norm": 6.109130382537842,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 7.2827,
      "step": 170
    },
    {
      "epoch": 0.17045454545454544,
      "grad_norm": 7.5872039794921875,
      "learning_rate": 1.8e-05,
      "loss": 7.2892,
      "step": 180
    },
    {
      "epoch": 0.17992424242424243,
      "grad_norm": 6.944248199462891,
      "learning_rate": 1.9e-05,
      "loss": 7.3479,
      "step": 190
    },
    {
      "epoch": 0.1893939393939394,
      "grad_norm": 6.2805633544921875,
      "learning_rate": 2e-05,
      "loss": 7.093,
      "step": 200
    },
    {
      "epoch": 0.19886363636363635,
      "grad_norm": 6.062386512756348,
      "learning_rate": 2.1e-05,
      "loss": 7.2039,
      "step": 210
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 6.833710193634033,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 7.0399,
      "step": 220
    },
    {
      "epoch": 0.2178030303030303,
      "grad_norm": 7.884421348571777,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 6.9234,
      "step": 230
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 6.430283069610596,
      "learning_rate": 2.4e-05,
      "loss": 7.0298,
      "step": 240
    },
    {
      "epoch": 0.23674242424242425,
      "grad_norm": 6.287380218505859,
      "learning_rate": 2.5e-05,
      "loss": 6.9053,
      "step": 250
    },
    {
      "epoch": 0.24621212121212122,
      "grad_norm": 7.785063743591309,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 6.8101,
      "step": 260
    },
    {
      "epoch": 0.2556818181818182,
      "grad_norm": 6.549957275390625,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 6.7156,
      "step": 270
    },
    {
      "epoch": 0.26515151515151514,
      "grad_norm": 9.753210067749023,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 6.7197,
      "step": 280
    },
    {
      "epoch": 0.2746212121212121,
      "grad_norm": 9.771408081054688,
      "learning_rate": 2.9e-05,
      "loss": 6.6801,
      "step": 290
    },
    {
      "epoch": 0.2840909090909091,
      "grad_norm": 6.681027412414551,
      "learning_rate": 3e-05,
      "loss": 6.5314,
      "step": 300
    },
    {
      "epoch": 0.2935606060606061,
      "grad_norm": 7.1100897789001465,
      "learning_rate": 3.1e-05,
      "loss": 6.4916,
      "step": 310
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 7.561883926391602,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 6.4474,
      "step": 320
    },
    {
      "epoch": 0.3125,
      "grad_norm": 7.0788116455078125,
      "learning_rate": 3.3e-05,
      "loss": 6.4519,
      "step": 330
    },
    {
      "epoch": 0.32196969696969696,
      "grad_norm": 7.395216464996338,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 6.4017,
      "step": 340
    },
    {
      "epoch": 0.3314393939393939,
      "grad_norm": 7.2384352684021,
      "learning_rate": 3.5e-05,
      "loss": 6.1625,
      "step": 350
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 6.132607460021973,
      "learning_rate": 3.6e-05,
      "loss": 6.1483,
      "step": 360
    },
    {
      "epoch": 0.3503787878787879,
      "grad_norm": 6.885608196258545,
      "learning_rate": 3.7e-05,
      "loss": 6.5869,
      "step": 370
    },
    {
      "epoch": 0.35984848484848486,
      "grad_norm": 6.510279178619385,
      "learning_rate": 3.8e-05,
      "loss": 6.0163,
      "step": 380
    },
    {
      "epoch": 0.3693181818181818,
      "grad_norm": 7.808743953704834,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 5.9766,
      "step": 390
    },
    {
      "epoch": 0.3787878787878788,
      "grad_norm": 6.35765266418457,
      "learning_rate": 4e-05,
      "loss": 6.0448,
      "step": 400
    },
    {
      "epoch": 0.38825757575757575,
      "grad_norm": 7.308446884155273,
      "learning_rate": 4.1e-05,
      "loss": 5.8903,
      "step": 410
    },
    {
      "epoch": 0.3977272727272727,
      "grad_norm": 7.121614456176758,
      "learning_rate": 4.2e-05,
      "loss": 5.7014,
      "step": 420
    },
    {
      "epoch": 0.4071969696969697,
      "grad_norm": 7.358831882476807,
      "learning_rate": 4.3e-05,
      "loss": 6.0353,
      "step": 430
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 6.656747817993164,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 6.1672,
      "step": 440
    },
    {
      "epoch": 0.42613636363636365,
      "grad_norm": 7.995197772979736,
      "learning_rate": 4.5e-05,
      "loss": 5.8981,
      "step": 450
    },
    {
      "epoch": 0.4356060606060606,
      "grad_norm": 7.981012344360352,
      "learning_rate": 4.600000000000001e-05,
      "loss": 5.8655,
      "step": 460
    },
    {
      "epoch": 0.44507575757575757,
      "grad_norm": 6.542821407318115,
      "learning_rate": 4.7e-05,
      "loss": 5.8792,
      "step": 470
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 6.521623134613037,
      "learning_rate": 4.8e-05,
      "loss": 6.0471,
      "step": 480
    },
    {
      "epoch": 0.4640151515151515,
      "grad_norm": 8.543663024902344,
      "learning_rate": 4.9e-05,
      "loss": 5.9857,
      "step": 490
    },
    {
      "epoch": 0.4734848484848485,
      "grad_norm": 7.958437442779541,
      "learning_rate": 5e-05,
      "loss": 5.6832,
      "step": 500
    },
    {
      "epoch": 0.48295454545454547,
      "grad_norm": 6.679714679718018,
      "learning_rate": 4.981259370314843e-05,
      "loss": 5.5621,
      "step": 510
    },
    {
      "epoch": 0.49242424242424243,
      "grad_norm": 7.683416366577148,
      "learning_rate": 4.9625187406296854e-05,
      "loss": 5.7933,
      "step": 520
    },
    {
      "epoch": 0.5018939393939394,
      "grad_norm": 7.517288684844971,
      "learning_rate": 4.943778110944528e-05,
      "loss": 5.5331,
      "step": 530
    },
    {
      "epoch": 0.5113636363636364,
      "grad_norm": 6.969518661499023,
      "learning_rate": 4.9250374812593707e-05,
      "loss": 5.726,
      "step": 540
    },
    {
      "epoch": 0.5208333333333334,
      "grad_norm": 6.542747974395752,
      "learning_rate": 4.906296851574213e-05,
      "loss": 5.6067,
      "step": 550
    },
    {
      "epoch": 0.5303030303030303,
      "grad_norm": 7.905175685882568,
      "learning_rate": 4.887556221889056e-05,
      "loss": 5.4347,
      "step": 560
    },
    {
      "epoch": 0.5397727272727273,
      "grad_norm": 6.2414469718933105,
      "learning_rate": 4.868815592203898e-05,
      "loss": 5.6306,
      "step": 570
    },
    {
      "epoch": 0.5492424242424242,
      "grad_norm": 6.946650505065918,
      "learning_rate": 4.850074962518741e-05,
      "loss": 5.2939,
      "step": 580
    },
    {
      "epoch": 0.5587121212121212,
      "grad_norm": 6.9763641357421875,
      "learning_rate": 4.831334332833584e-05,
      "loss": 5.6646,
      "step": 590
    },
    {
      "epoch": 0.5681818181818182,
      "grad_norm": 7.043882369995117,
      "learning_rate": 4.8125937031484256e-05,
      "loss": 5.2852,
      "step": 600
    },
    {
      "epoch": 0.5776515151515151,
      "grad_norm": 7.265847682952881,
      "learning_rate": 4.793853073463269e-05,
      "loss": 5.4791,
      "step": 610
    },
    {
      "epoch": 0.5871212121212122,
      "grad_norm": 7.872783660888672,
      "learning_rate": 4.7751124437781115e-05,
      "loss": 5.6164,
      "step": 620
    },
    {
      "epoch": 0.5965909090909091,
      "grad_norm": 6.840357303619385,
      "learning_rate": 4.7563718140929534e-05,
      "loss": 5.7743,
      "step": 630
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 6.094076633453369,
      "learning_rate": 4.737631184407797e-05,
      "loss": 5.2688,
      "step": 640
    },
    {
      "epoch": 0.615530303030303,
      "grad_norm": 6.337645530700684,
      "learning_rate": 4.7188905547226386e-05,
      "loss": 5.5651,
      "step": 650
    },
    {
      "epoch": 0.625,
      "grad_norm": 6.945743560791016,
      "learning_rate": 4.700149925037481e-05,
      "loss": 5.2959,
      "step": 660
    },
    {
      "epoch": 0.634469696969697,
      "grad_norm": 7.10670280456543,
      "learning_rate": 4.6814092953523245e-05,
      "loss": 5.2665,
      "step": 670
    },
    {
      "epoch": 0.6439393939393939,
      "grad_norm": 7.408449649810791,
      "learning_rate": 4.6626686656671664e-05,
      "loss": 5.1374,
      "step": 680
    },
    {
      "epoch": 0.6534090909090909,
      "grad_norm": 7.03241491317749,
      "learning_rate": 4.643928035982009e-05,
      "loss": 5.4926,
      "step": 690
    },
    {
      "epoch": 0.6628787878787878,
      "grad_norm": 8.924760818481445,
      "learning_rate": 4.625187406296852e-05,
      "loss": 5.4094,
      "step": 700
    },
    {
      "epoch": 0.6723484848484849,
      "grad_norm": 7.036317825317383,
      "learning_rate": 4.606446776611694e-05,
      "loss": 5.0277,
      "step": 710
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 6.019045829772949,
      "learning_rate": 4.587706146926537e-05,
      "loss": 5.0061,
      "step": 720
    },
    {
      "epoch": 0.6912878787878788,
      "grad_norm": 6.536244869232178,
      "learning_rate": 4.5689655172413794e-05,
      "loss": 5.1856,
      "step": 730
    },
    {
      "epoch": 0.7007575757575758,
      "grad_norm": 6.105303764343262,
      "learning_rate": 4.550224887556222e-05,
      "loss": 5.0087,
      "step": 740
    },
    {
      "epoch": 0.7102272727272727,
      "grad_norm": 6.735917568206787,
      "learning_rate": 4.5314842578710647e-05,
      "loss": 4.8505,
      "step": 750
    },
    {
      "epoch": 0.7196969696969697,
      "grad_norm": 7.213285446166992,
      "learning_rate": 4.512743628185907e-05,
      "loss": 4.9663,
      "step": 760
    },
    {
      "epoch": 0.7291666666666666,
      "grad_norm": 7.261063575744629,
      "learning_rate": 4.49400299850075e-05,
      "loss": 4.8521,
      "step": 770
    },
    {
      "epoch": 0.7386363636363636,
      "grad_norm": 6.514797210693359,
      "learning_rate": 4.4752623688155925e-05,
      "loss": 5.2008,
      "step": 780
    },
    {
      "epoch": 0.7481060606060606,
      "grad_norm": 6.004269123077393,
      "learning_rate": 4.456521739130435e-05,
      "loss": 4.774,
      "step": 790
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 6.766750812530518,
      "learning_rate": 4.437781109445278e-05,
      "loss": 4.9474,
      "step": 800
    },
    {
      "epoch": 0.7670454545454546,
      "grad_norm": 6.643136501312256,
      "learning_rate": 4.41904047976012e-05,
      "loss": 4.9185,
      "step": 810
    },
    {
      "epoch": 0.7765151515151515,
      "grad_norm": 6.803291320800781,
      "learning_rate": 4.400299850074963e-05,
      "loss": 4.8004,
      "step": 820
    },
    {
      "epoch": 0.7859848484848485,
      "grad_norm": 6.032364368438721,
      "learning_rate": 4.3815592203898055e-05,
      "loss": 4.6308,
      "step": 830
    },
    {
      "epoch": 0.7954545454545454,
      "grad_norm": 6.395646572113037,
      "learning_rate": 4.362818590704648e-05,
      "loss": 4.8925,
      "step": 840
    },
    {
      "epoch": 0.8049242424242424,
      "grad_norm": 8.406516075134277,
      "learning_rate": 4.344077961019491e-05,
      "loss": 4.848,
      "step": 850
    },
    {
      "epoch": 0.8143939393939394,
      "grad_norm": 5.931242942810059,
      "learning_rate": 4.325337331334333e-05,
      "loss": 4.9827,
      "step": 860
    },
    {
      "epoch": 0.8238636363636364,
      "grad_norm": 5.849670886993408,
      "learning_rate": 4.306596701649175e-05,
      "loss": 4.7777,
      "step": 870
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 6.625092506408691,
      "learning_rate": 4.2878560719640185e-05,
      "loss": 4.4646,
      "step": 880
    },
    {
      "epoch": 0.8428030303030303,
      "grad_norm": 8.0374755859375,
      "learning_rate": 4.269115442278861e-05,
      "loss": 4.2422,
      "step": 890
    },
    {
      "epoch": 0.8522727272727273,
      "grad_norm": 5.9103922843933105,
      "learning_rate": 4.250374812593703e-05,
      "loss": 4.9539,
      "step": 900
    },
    {
      "epoch": 0.8617424242424242,
      "grad_norm": 6.575414657592773,
      "learning_rate": 4.2316341829085456e-05,
      "loss": 5.0569,
      "step": 910
    },
    {
      "epoch": 0.8712121212121212,
      "grad_norm": 6.541769504547119,
      "learning_rate": 4.212893553223389e-05,
      "loss": 4.5298,
      "step": 920
    },
    {
      "epoch": 0.8806818181818182,
      "grad_norm": 7.611377716064453,
      "learning_rate": 4.194152923538231e-05,
      "loss": 4.5547,
      "step": 930
    },
    {
      "epoch": 0.8901515151515151,
      "grad_norm": 7.27893590927124,
      "learning_rate": 4.1754122938530734e-05,
      "loss": 4.3113,
      "step": 940
    },
    {
      "epoch": 0.8996212121212122,
      "grad_norm": 5.862819671630859,
      "learning_rate": 4.156671664167916e-05,
      "loss": 4.4952,
      "step": 950
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 7.000759124755859,
      "learning_rate": 4.1379310344827587e-05,
      "loss": 4.6168,
      "step": 960
    },
    {
      "epoch": 0.9185606060606061,
      "grad_norm": 6.774171829223633,
      "learning_rate": 4.119190404797601e-05,
      "loss": 4.3312,
      "step": 970
    },
    {
      "epoch": 0.928030303030303,
      "grad_norm": 5.737522602081299,
      "learning_rate": 4.100449775112444e-05,
      "loss": 4.5722,
      "step": 980
    },
    {
      "epoch": 0.9375,
      "grad_norm": 7.043646335601807,
      "learning_rate": 4.0817091454272865e-05,
      "loss": 4.8524,
      "step": 990
    },
    {
      "epoch": 0.946969696969697,
      "grad_norm": 8.284098625183105,
      "learning_rate": 4.062968515742129e-05,
      "loss": 4.5719,
      "step": 1000
    },
    {
      "epoch": 0.9564393939393939,
      "grad_norm": 8.269354820251465,
      "learning_rate": 4.044227886056972e-05,
      "loss": 4.2135,
      "step": 1010
    },
    {
      "epoch": 0.9659090909090909,
      "grad_norm": 6.298093795776367,
      "learning_rate": 4.025487256371814e-05,
      "loss": 4.2838,
      "step": 1020
    },
    {
      "epoch": 0.9753787878787878,
      "grad_norm": 6.544422149658203,
      "learning_rate": 4.006746626686657e-05,
      "loss": 4.398,
      "step": 1030
    },
    {
      "epoch": 0.9848484848484849,
      "grad_norm": 6.193176746368408,
      "learning_rate": 3.9880059970014995e-05,
      "loss": 4.2281,
      "step": 1040
    },
    {
      "epoch": 0.9943181818181818,
      "grad_norm": 7.387221813201904,
      "learning_rate": 3.969265367316342e-05,
      "loss": 4.6078,
      "step": 1050
    },
    {
      "epoch": 1.003787878787879,
      "grad_norm": 5.906759738922119,
      "learning_rate": 3.950524737631185e-05,
      "loss": 4.4211,
      "step": 1060
    },
    {
      "epoch": 1.0132575757575757,
      "grad_norm": 6.030951023101807,
      "learning_rate": 3.931784107946027e-05,
      "loss": 4.404,
      "step": 1070
    },
    {
      "epoch": 1.0227272727272727,
      "grad_norm": 6.0546183586120605,
      "learning_rate": 3.91304347826087e-05,
      "loss": 4.089,
      "step": 1080
    },
    {
      "epoch": 1.0321969696969697,
      "grad_norm": 6.389708995819092,
      "learning_rate": 3.894302848575712e-05,
      "loss": 4.1644,
      "step": 1090
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 5.592456340789795,
      "learning_rate": 3.875562218890555e-05,
      "loss": 4.0612,
      "step": 1100
    },
    {
      "epoch": 1.0511363636363635,
      "grad_norm": 6.156928062438965,
      "learning_rate": 3.856821589205398e-05,
      "loss": 4.1758,
      "step": 1110
    },
    {
      "epoch": 1.0606060606060606,
      "grad_norm": 5.32877779006958,
      "learning_rate": 3.8380809595202396e-05,
      "loss": 3.5751,
      "step": 1120
    },
    {
      "epoch": 1.0700757575757576,
      "grad_norm": 6.489378452301025,
      "learning_rate": 3.819340329835083e-05,
      "loss": 3.9056,
      "step": 1130
    },
    {
      "epoch": 1.0795454545454546,
      "grad_norm": 5.106256008148193,
      "learning_rate": 3.800599700149925e-05,
      "loss": 3.849,
      "step": 1140
    },
    {
      "epoch": 1.0890151515151516,
      "grad_norm": 6.031830787658691,
      "learning_rate": 3.7818590704647674e-05,
      "loss": 4.2168,
      "step": 1150
    },
    {
      "epoch": 1.0984848484848484,
      "grad_norm": 5.547618865966797,
      "learning_rate": 3.763118440779611e-05,
      "loss": 4.154,
      "step": 1160
    },
    {
      "epoch": 1.1079545454545454,
      "grad_norm": 6.38686466217041,
      "learning_rate": 3.7443778110944527e-05,
      "loss": 4.3315,
      "step": 1170
    },
    {
      "epoch": 1.1174242424242424,
      "grad_norm": 5.773675918579102,
      "learning_rate": 3.725637181409295e-05,
      "loss": 3.8339,
      "step": 1180
    },
    {
      "epoch": 1.1268939393939394,
      "grad_norm": 5.585844993591309,
      "learning_rate": 3.7068965517241385e-05,
      "loss": 4.1879,
      "step": 1190
    },
    {
      "epoch": 1.1363636363636362,
      "grad_norm": 6.225644588470459,
      "learning_rate": 3.6881559220389805e-05,
      "loss": 3.8732,
      "step": 1200
    },
    {
      "epoch": 1.1458333333333333,
      "grad_norm": 5.084029674530029,
      "learning_rate": 3.669415292353823e-05,
      "loss": 3.6619,
      "step": 1210
    },
    {
      "epoch": 1.1553030303030303,
      "grad_norm": 5.730865955352783,
      "learning_rate": 3.6506746626686664e-05,
      "loss": 3.4447,
      "step": 1220
    },
    {
      "epoch": 1.1647727272727273,
      "grad_norm": 5.889404296875,
      "learning_rate": 3.631934032983508e-05,
      "loss": 4.0471,
      "step": 1230
    },
    {
      "epoch": 1.1742424242424243,
      "grad_norm": 6.050325870513916,
      "learning_rate": 3.613193403298351e-05,
      "loss": 4.0325,
      "step": 1240
    },
    {
      "epoch": 1.183712121212121,
      "grad_norm": 6.437356472015381,
      "learning_rate": 3.5944527736131935e-05,
      "loss": 3.9727,
      "step": 1250
    },
    {
      "epoch": 1.1931818181818181,
      "grad_norm": 5.412191867828369,
      "learning_rate": 3.575712143928036e-05,
      "loss": 3.8447,
      "step": 1260
    },
    {
      "epoch": 1.2026515151515151,
      "grad_norm": 6.194351673126221,
      "learning_rate": 3.556971514242879e-05,
      "loss": 3.622,
      "step": 1270
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 6.798957347869873,
      "learning_rate": 3.538230884557721e-05,
      "loss": 3.5493,
      "step": 1280
    },
    {
      "epoch": 1.2215909090909092,
      "grad_norm": 6.060061931610107,
      "learning_rate": 3.519490254872564e-05,
      "loss": 3.9979,
      "step": 1290
    },
    {
      "epoch": 1.231060606060606,
      "grad_norm": 6.122771263122559,
      "learning_rate": 3.5007496251874065e-05,
      "loss": 3.3382,
      "step": 1300
    },
    {
      "epoch": 1.240530303030303,
      "grad_norm": 7.04654598236084,
      "learning_rate": 3.482008995502249e-05,
      "loss": 3.8127,
      "step": 1310
    },
    {
      "epoch": 1.25,
      "grad_norm": 5.253035545349121,
      "learning_rate": 3.463268365817092e-05,
      "loss": 3.7328,
      "step": 1320
    },
    {
      "epoch": 1.259469696969697,
      "grad_norm": 5.771172523498535,
      "learning_rate": 3.444527736131934e-05,
      "loss": 3.8643,
      "step": 1330
    },
    {
      "epoch": 1.268939393939394,
      "grad_norm": 5.66810417175293,
      "learning_rate": 3.425787106446777e-05,
      "loss": 3.592,
      "step": 1340
    },
    {
      "epoch": 1.2784090909090908,
      "grad_norm": 6.886592388153076,
      "learning_rate": 3.4070464767616195e-05,
      "loss": 4.3341,
      "step": 1350
    },
    {
      "epoch": 1.2878787878787878,
      "grad_norm": 6.600263595581055,
      "learning_rate": 3.3883058470764614e-05,
      "loss": 3.5376,
      "step": 1360
    },
    {
      "epoch": 1.2973484848484849,
      "grad_norm": 6.49062967300415,
      "learning_rate": 3.369565217391305e-05,
      "loss": 3.7976,
      "step": 1370
    },
    {
      "epoch": 1.3068181818181819,
      "grad_norm": 5.923972129821777,
      "learning_rate": 3.350824587706147e-05,
      "loss": 3.7152,
      "step": 1380
    },
    {
      "epoch": 1.316287878787879,
      "grad_norm": 5.967902183532715,
      "learning_rate": 3.332083958020989e-05,
      "loss": 3.3953,
      "step": 1390
    },
    {
      "epoch": 1.3257575757575757,
      "grad_norm": 5.5085978507995605,
      "learning_rate": 3.3133433283358325e-05,
      "loss": 3.7844,
      "step": 1400
    },
    {
      "epoch": 1.3352272727272727,
      "grad_norm": 4.966441631317139,
      "learning_rate": 3.294602698650675e-05,
      "loss": 3.7177,
      "step": 1410
    },
    {
      "epoch": 1.3446969696969697,
      "grad_norm": 7.042933464050293,
      "learning_rate": 3.275862068965517e-05,
      "loss": 3.636,
      "step": 1420
    },
    {
      "epoch": 1.3541666666666667,
      "grad_norm": 5.390649795532227,
      "learning_rate": 3.2571214392803604e-05,
      "loss": 3.6849,
      "step": 1430
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 6.677230358123779,
      "learning_rate": 3.238380809595202e-05,
      "loss": 4.2375,
      "step": 1440
    },
    {
      "epoch": 1.3731060606060606,
      "grad_norm": 6.681018352508545,
      "learning_rate": 3.219640179910045e-05,
      "loss": 3.8754,
      "step": 1450
    },
    {
      "epoch": 1.3825757575757576,
      "grad_norm": 6.497195720672607,
      "learning_rate": 3.200899550224888e-05,
      "loss": 3.5449,
      "step": 1460
    },
    {
      "epoch": 1.3920454545454546,
      "grad_norm": 9.383238792419434,
      "learning_rate": 3.18215892053973e-05,
      "loss": 3.598,
      "step": 1470
    },
    {
      "epoch": 1.4015151515151514,
      "grad_norm": 6.048605918884277,
      "learning_rate": 3.163418290854573e-05,
      "loss": 3.551,
      "step": 1480
    },
    {
      "epoch": 1.4109848484848486,
      "grad_norm": 5.192832946777344,
      "learning_rate": 3.144677661169416e-05,
      "loss": 3.5302,
      "step": 1490
    },
    {
      "epoch": 1.4204545454545454,
      "grad_norm": 6.714267253875732,
      "learning_rate": 3.125937031484258e-05,
      "loss": 3.5077,
      "step": 1500
    },
    {
      "epoch": 1.4299242424242424,
      "grad_norm": 5.7662153244018555,
      "learning_rate": 3.1071964017991005e-05,
      "loss": 3.4242,
      "step": 1510
    },
    {
      "epoch": 1.4393939393939394,
      "grad_norm": 5.900445938110352,
      "learning_rate": 3.088455772113943e-05,
      "loss": 3.5308,
      "step": 1520
    },
    {
      "epoch": 1.4488636363636362,
      "grad_norm": 6.635690212249756,
      "learning_rate": 3.069715142428786e-05,
      "loss": 3.503,
      "step": 1530
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 5.096013069152832,
      "learning_rate": 3.0509745127436283e-05,
      "loss": 3.4823,
      "step": 1540
    },
    {
      "epoch": 1.4678030303030303,
      "grad_norm": 5.249617576599121,
      "learning_rate": 3.0322338830584706e-05,
      "loss": 3.3555,
      "step": 1550
    },
    {
      "epoch": 1.4772727272727273,
      "grad_norm": 6.3736114501953125,
      "learning_rate": 3.0134932533733135e-05,
      "loss": 3.3595,
      "step": 1560
    },
    {
      "epoch": 1.4867424242424243,
      "grad_norm": 6.258951663970947,
      "learning_rate": 2.994752623688156e-05,
      "loss": 3.4745,
      "step": 1570
    },
    {
      "epoch": 1.496212121212121,
      "grad_norm": 6.015310764312744,
      "learning_rate": 2.9760119940029984e-05,
      "loss": 3.6267,
      "step": 1580
    },
    {
      "epoch": 1.5056818181818183,
      "grad_norm": 5.142882823944092,
      "learning_rate": 2.9572713643178413e-05,
      "loss": 3.5302,
      "step": 1590
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 5.635993480682373,
      "learning_rate": 2.938530734632684e-05,
      "loss": 3.3292,
      "step": 1600
    },
    {
      "epoch": 1.5246212121212122,
      "grad_norm": 5.7774128913879395,
      "learning_rate": 2.9197901049475262e-05,
      "loss": 3.5275,
      "step": 1610
    },
    {
      "epoch": 1.5340909090909092,
      "grad_norm": 4.700299263000488,
      "learning_rate": 2.901049475262369e-05,
      "loss": 3.2101,
      "step": 1620
    },
    {
      "epoch": 1.543560606060606,
      "grad_norm": 5.176416873931885,
      "learning_rate": 2.8823088455772114e-05,
      "loss": 3.4212,
      "step": 1630
    },
    {
      "epoch": 1.553030303030303,
      "grad_norm": 6.1848225593566895,
      "learning_rate": 2.863568215892054e-05,
      "loss": 3.3207,
      "step": 1640
    },
    {
      "epoch": 1.5625,
      "grad_norm": 6.085071086883545,
      "learning_rate": 2.844827586206897e-05,
      "loss": 3.4251,
      "step": 1650
    },
    {
      "epoch": 1.571969696969697,
      "grad_norm": 6.925287246704102,
      "learning_rate": 2.826086956521739e-05,
      "loss": 3.3674,
      "step": 1660
    },
    {
      "epoch": 1.581439393939394,
      "grad_norm": 6.340954303741455,
      "learning_rate": 2.8073463268365818e-05,
      "loss": 3.3534,
      "step": 1670
    },
    {
      "epoch": 1.5909090909090908,
      "grad_norm": 5.814598560333252,
      "learning_rate": 2.7886056971514248e-05,
      "loss": 3.5555,
      "step": 1680
    },
    {
      "epoch": 1.6003787878787878,
      "grad_norm": 5.520164966583252,
      "learning_rate": 2.7698650674662667e-05,
      "loss": 3.4523,
      "step": 1690
    },
    {
      "epoch": 1.6098484848484849,
      "grad_norm": 7.1984453201293945,
      "learning_rate": 2.7511244377811096e-05,
      "loss": 3.5891,
      "step": 1700
    },
    {
      "epoch": 1.6193181818181817,
      "grad_norm": 6.423941612243652,
      "learning_rate": 2.7323838080959522e-05,
      "loss": 3.526,
      "step": 1710
    },
    {
      "epoch": 1.628787878787879,
      "grad_norm": 6.62019157409668,
      "learning_rate": 2.7136431784107945e-05,
      "loss": 3.3525,
      "step": 1720
    },
    {
      "epoch": 1.6382575757575757,
      "grad_norm": 6.651606559753418,
      "learning_rate": 2.6949025487256374e-05,
      "loss": 3.2879,
      "step": 1730
    },
    {
      "epoch": 1.6477272727272727,
      "grad_norm": 5.742660999298096,
      "learning_rate": 2.6761619190404797e-05,
      "loss": 3.7229,
      "step": 1740
    },
    {
      "epoch": 1.6571969696969697,
      "grad_norm": 4.835978031158447,
      "learning_rate": 2.6574212893553223e-05,
      "loss": 3.4541,
      "step": 1750
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 6.74519681930542,
      "learning_rate": 2.6386806596701653e-05,
      "loss": 3.3816,
      "step": 1760
    },
    {
      "epoch": 1.6761363636363638,
      "grad_norm": 5.861912250518799,
      "learning_rate": 2.6199400299850075e-05,
      "loss": 3.5292,
      "step": 1770
    },
    {
      "epoch": 1.6856060606060606,
      "grad_norm": 7.892606258392334,
      "learning_rate": 2.60119940029985e-05,
      "loss": 3.4383,
      "step": 1780
    },
    {
      "epoch": 1.6950757575757576,
      "grad_norm": 7.841166019439697,
      "learning_rate": 2.582458770614693e-05,
      "loss": 3.6008,
      "step": 1790
    },
    {
      "epoch": 1.7045454545454546,
      "grad_norm": 5.753739356994629,
      "learning_rate": 2.5637181409295353e-05,
      "loss": 3.595,
      "step": 1800
    },
    {
      "epoch": 1.7140151515151514,
      "grad_norm": 5.48687219619751,
      "learning_rate": 2.544977511244378e-05,
      "loss": 2.9905,
      "step": 1810
    },
    {
      "epoch": 1.7234848484848486,
      "grad_norm": 6.128905296325684,
      "learning_rate": 2.526236881559221e-05,
      "loss": 3.2034,
      "step": 1820
    },
    {
      "epoch": 1.7329545454545454,
      "grad_norm": 5.272014617919922,
      "learning_rate": 2.507496251874063e-05,
      "loss": 3.2312,
      "step": 1830
    },
    {
      "epoch": 1.7424242424242424,
      "grad_norm": 5.8412089347839355,
      "learning_rate": 2.4887556221889057e-05,
      "loss": 3.636,
      "step": 1840
    },
    {
      "epoch": 1.7518939393939394,
      "grad_norm": 5.816162109375,
      "learning_rate": 2.4700149925037484e-05,
      "loss": 3.1843,
      "step": 1850
    },
    {
      "epoch": 1.7613636363636362,
      "grad_norm": 6.3604416847229,
      "learning_rate": 2.451274362818591e-05,
      "loss": 3.1562,
      "step": 1860
    },
    {
      "epoch": 1.7708333333333335,
      "grad_norm": 5.8037896156311035,
      "learning_rate": 2.4325337331334332e-05,
      "loss": 2.8316,
      "step": 1870
    },
    {
      "epoch": 1.7803030303030303,
      "grad_norm": 6.489089012145996,
      "learning_rate": 2.413793103448276e-05,
      "loss": 3.0775,
      "step": 1880
    },
    {
      "epoch": 1.7897727272727273,
      "grad_norm": 5.970987319946289,
      "learning_rate": 2.3950524737631184e-05,
      "loss": 3.3875,
      "step": 1890
    },
    {
      "epoch": 1.7992424242424243,
      "grad_norm": 6.488520622253418,
      "learning_rate": 2.376311844077961e-05,
      "loss": 3.6593,
      "step": 1900
    },
    {
      "epoch": 1.808712121212121,
      "grad_norm": 5.2918267250061035,
      "learning_rate": 2.3575712143928036e-05,
      "loss": 3.0576,
      "step": 1910
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 6.339167594909668,
      "learning_rate": 2.3388305847076462e-05,
      "loss": 3.4077,
      "step": 1920
    },
    {
      "epoch": 1.8276515151515151,
      "grad_norm": 5.332231521606445,
      "learning_rate": 2.320089955022489e-05,
      "loss": 2.926,
      "step": 1930
    },
    {
      "epoch": 1.8371212121212122,
      "grad_norm": 5.914253234863281,
      "learning_rate": 2.3013493253373314e-05,
      "loss": 3.0413,
      "step": 1940
    },
    {
      "epoch": 1.8465909090909092,
      "grad_norm": 6.741146564483643,
      "learning_rate": 2.282608695652174e-05,
      "loss": 2.9647,
      "step": 1950
    },
    {
      "epoch": 1.856060606060606,
      "grad_norm": 5.6267409324646,
      "learning_rate": 2.2638680659670167e-05,
      "loss": 3.3739,
      "step": 1960
    },
    {
      "epoch": 1.865530303030303,
      "grad_norm": 5.96769905090332,
      "learning_rate": 2.2451274362818593e-05,
      "loss": 2.8536,
      "step": 1970
    },
    {
      "epoch": 1.875,
      "grad_norm": 5.655837059020996,
      "learning_rate": 2.2263868065967015e-05,
      "loss": 3.5562,
      "step": 1980
    },
    {
      "epoch": 1.884469696969697,
      "grad_norm": 6.95176362991333,
      "learning_rate": 2.2076461769115445e-05,
      "loss": 3.4474,
      "step": 1990
    },
    {
      "epoch": 1.893939393939394,
      "grad_norm": 6.0863165855407715,
      "learning_rate": 2.188905547226387e-05,
      "loss": 3.1635,
      "step": 2000
    },
    {
      "epoch": 1.9034090909090908,
      "grad_norm": 5.746358871459961,
      "learning_rate": 2.1701649175412293e-05,
      "loss": 3.3267,
      "step": 2010
    },
    {
      "epoch": 1.9128787878787878,
      "grad_norm": 6.580296516418457,
      "learning_rate": 2.151424287856072e-05,
      "loss": 3.2866,
      "step": 2020
    },
    {
      "epoch": 1.9223484848484849,
      "grad_norm": 5.130228519439697,
      "learning_rate": 2.132683658170915e-05,
      "loss": 3.4782,
      "step": 2030
    },
    {
      "epoch": 1.9318181818181817,
      "grad_norm": 5.446500301361084,
      "learning_rate": 2.113943028485757e-05,
      "loss": 3.0664,
      "step": 2040
    },
    {
      "epoch": 1.941287878787879,
      "grad_norm": 5.950015068054199,
      "learning_rate": 2.0952023988005998e-05,
      "loss": 3.2039,
      "step": 2050
    },
    {
      "epoch": 1.9507575757575757,
      "grad_norm": 6.446861267089844,
      "learning_rate": 2.0764617691154424e-05,
      "loss": 3.2187,
      "step": 2060
    },
    {
      "epoch": 1.9602272727272727,
      "grad_norm": 5.329479217529297,
      "learning_rate": 2.057721139430285e-05,
      "loss": 2.8737,
      "step": 2070
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 6.331442832946777,
      "learning_rate": 2.0389805097451276e-05,
      "loss": 3.0145,
      "step": 2080
    },
    {
      "epoch": 1.9791666666666665,
      "grad_norm": 6.051646709442139,
      "learning_rate": 2.02023988005997e-05,
      "loss": 3.3935,
      "step": 2090
    },
    {
      "epoch": 1.9886363636363638,
      "grad_norm": 7.010474681854248,
      "learning_rate": 2.0014992503748124e-05,
      "loss": 2.9478,
      "step": 2100
    },
    {
      "epoch": 1.9981060606060606,
      "grad_norm": 5.914651393890381,
      "learning_rate": 1.9827586206896554e-05,
      "loss": 2.7887,
      "step": 2110
    },
    {
      "epoch": 2.007575757575758,
      "grad_norm": 6.042084693908691,
      "learning_rate": 1.964017991004498e-05,
      "loss": 2.8891,
      "step": 2120
    },
    {
      "epoch": 2.0170454545454546,
      "grad_norm": 5.386901378631592,
      "learning_rate": 1.9452773613193402e-05,
      "loss": 3.0698,
      "step": 2130
    },
    {
      "epoch": 2.0265151515151514,
      "grad_norm": 5.6382856369018555,
      "learning_rate": 1.9265367316341832e-05,
      "loss": 2.4839,
      "step": 2140
    },
    {
      "epoch": 2.0359848484848486,
      "grad_norm": 4.79237699508667,
      "learning_rate": 1.9077961019490258e-05,
      "loss": 2.9058,
      "step": 2150
    },
    {
      "epoch": 2.0454545454545454,
      "grad_norm": 6.29052734375,
      "learning_rate": 1.889055472263868e-05,
      "loss": 3.0182,
      "step": 2160
    },
    {
      "epoch": 2.054924242424242,
      "grad_norm": 5.95599365234375,
      "learning_rate": 1.8703148425787107e-05,
      "loss": 2.7865,
      "step": 2170
    },
    {
      "epoch": 2.0643939393939394,
      "grad_norm": 6.396402835845947,
      "learning_rate": 1.8515742128935533e-05,
      "loss": 3.0136,
      "step": 2180
    },
    {
      "epoch": 2.0738636363636362,
      "grad_norm": 5.635098457336426,
      "learning_rate": 1.832833583208396e-05,
      "loss": 2.971,
      "step": 2190
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 5.595417499542236,
      "learning_rate": 1.8140929535232385e-05,
      "loss": 2.7919,
      "step": 2200
    },
    {
      "epoch": 2.0928030303030303,
      "grad_norm": 6.029301166534424,
      "learning_rate": 1.795352323838081e-05,
      "loss": 3.3253,
      "step": 2210
    },
    {
      "epoch": 2.102272727272727,
      "grad_norm": 6.574413776397705,
      "learning_rate": 1.7766116941529237e-05,
      "loss": 3.2486,
      "step": 2220
    },
    {
      "epoch": 2.1117424242424243,
      "grad_norm": 4.462207794189453,
      "learning_rate": 1.7578710644677663e-05,
      "loss": 2.813,
      "step": 2230
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 4.963842868804932,
      "learning_rate": 1.739130434782609e-05,
      "loss": 2.8445,
      "step": 2240
    },
    {
      "epoch": 2.1306818181818183,
      "grad_norm": 4.7952961921691895,
      "learning_rate": 1.720389805097451e-05,
      "loss": 2.8741,
      "step": 2250
    },
    {
      "epoch": 2.140151515151515,
      "grad_norm": 6.279271602630615,
      "learning_rate": 1.701649175412294e-05,
      "loss": 3.052,
      "step": 2260
    },
    {
      "epoch": 2.149621212121212,
      "grad_norm": 5.666098594665527,
      "learning_rate": 1.6829085457271364e-05,
      "loss": 2.7666,
      "step": 2270
    },
    {
      "epoch": 2.159090909090909,
      "grad_norm": 5.329780578613281,
      "learning_rate": 1.664167916041979e-05,
      "loss": 2.7098,
      "step": 2280
    },
    {
      "epoch": 2.168560606060606,
      "grad_norm": 6.305290222167969,
      "learning_rate": 1.645427286356822e-05,
      "loss": 2.7906,
      "step": 2290
    },
    {
      "epoch": 2.178030303030303,
      "grad_norm": 6.560731410980225,
      "learning_rate": 1.626686656671664e-05,
      "loss": 2.7017,
      "step": 2300
    },
    {
      "epoch": 2.1875,
      "grad_norm": 6.052905082702637,
      "learning_rate": 1.6079460269865068e-05,
      "loss": 3.0349,
      "step": 2310
    },
    {
      "epoch": 2.196969696969697,
      "grad_norm": 6.6413373947143555,
      "learning_rate": 1.5892053973013494e-05,
      "loss": 2.9482,
      "step": 2320
    },
    {
      "epoch": 2.206439393939394,
      "grad_norm": 4.610114574432373,
      "learning_rate": 1.570464767616192e-05,
      "loss": 2.758,
      "step": 2330
    },
    {
      "epoch": 2.215909090909091,
      "grad_norm": 5.4355645179748535,
      "learning_rate": 1.5517241379310346e-05,
      "loss": 2.895,
      "step": 2340
    },
    {
      "epoch": 2.225378787878788,
      "grad_norm": 5.522250175476074,
      "learning_rate": 1.5329835082458772e-05,
      "loss": 2.7696,
      "step": 2350
    },
    {
      "epoch": 2.234848484848485,
      "grad_norm": 5.539608001708984,
      "learning_rate": 1.5142428785607196e-05,
      "loss": 2.6451,
      "step": 2360
    },
    {
      "epoch": 2.2443181818181817,
      "grad_norm": 6.592518329620361,
      "learning_rate": 1.4955022488755624e-05,
      "loss": 2.8134,
      "step": 2370
    },
    {
      "epoch": 2.253787878787879,
      "grad_norm": 5.092324256896973,
      "learning_rate": 1.4767616191904048e-05,
      "loss": 2.5168,
      "step": 2380
    },
    {
      "epoch": 2.2632575757575757,
      "grad_norm": 5.5535969734191895,
      "learning_rate": 1.4580209895052474e-05,
      "loss": 2.7023,
      "step": 2390
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 5.122145652770996,
      "learning_rate": 1.4392803598200899e-05,
      "loss": 2.8122,
      "step": 2400
    },
    {
      "epoch": 2.2821969696969697,
      "grad_norm": 5.531137466430664,
      "learning_rate": 1.4205397301349326e-05,
      "loss": 2.6781,
      "step": 2410
    },
    {
      "epoch": 2.2916666666666665,
      "grad_norm": 5.254491806030273,
      "learning_rate": 1.4017991004497752e-05,
      "loss": 2.461,
      "step": 2420
    },
    {
      "epoch": 2.3011363636363638,
      "grad_norm": 5.290779113769531,
      "learning_rate": 1.3830584707646177e-05,
      "loss": 2.9643,
      "step": 2430
    },
    {
      "epoch": 2.3106060606060606,
      "grad_norm": 6.929267883300781,
      "learning_rate": 1.3643178410794603e-05,
      "loss": 2.8625,
      "step": 2440
    },
    {
      "epoch": 2.320075757575758,
      "grad_norm": 5.656546115875244,
      "learning_rate": 1.345577211394303e-05,
      "loss": 3.0991,
      "step": 2450
    },
    {
      "epoch": 2.3295454545454546,
      "grad_norm": 4.3472700119018555,
      "learning_rate": 1.3268365817091455e-05,
      "loss": 2.4634,
      "step": 2460
    },
    {
      "epoch": 2.3390151515151514,
      "grad_norm": 6.146081447601318,
      "learning_rate": 1.308095952023988e-05,
      "loss": 2.9608,
      "step": 2470
    },
    {
      "epoch": 2.3484848484848486,
      "grad_norm": 4.398021221160889,
      "learning_rate": 1.2893553223388307e-05,
      "loss": 2.4502,
      "step": 2480
    },
    {
      "epoch": 2.3579545454545454,
      "grad_norm": 4.987365245819092,
      "learning_rate": 1.2706146926536733e-05,
      "loss": 3.0355,
      "step": 2490
    },
    {
      "epoch": 2.367424242424242,
      "grad_norm": 6.839677810668945,
      "learning_rate": 1.2518740629685157e-05,
      "loss": 3.2093,
      "step": 2500
    },
    {
      "epoch": 2.3768939393939394,
      "grad_norm": 5.905550956726074,
      "learning_rate": 1.2331334332833585e-05,
      "loss": 3.0052,
      "step": 2510
    },
    {
      "epoch": 2.3863636363636362,
      "grad_norm": 6.030330181121826,
      "learning_rate": 1.214392803598201e-05,
      "loss": 3.0112,
      "step": 2520
    },
    {
      "epoch": 2.3958333333333335,
      "grad_norm": 5.432827949523926,
      "learning_rate": 1.1956521739130435e-05,
      "loss": 2.7077,
      "step": 2530
    },
    {
      "epoch": 2.4053030303030303,
      "grad_norm": 8.42021656036377,
      "learning_rate": 1.1769115442278861e-05,
      "loss": 2.8041,
      "step": 2540
    },
    {
      "epoch": 2.4147727272727275,
      "grad_norm": 5.123525142669678,
      "learning_rate": 1.1581709145427288e-05,
      "loss": 2.9952,
      "step": 2550
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 5.892205715179443,
      "learning_rate": 1.1394302848575712e-05,
      "loss": 2.8139,
      "step": 2560
    },
    {
      "epoch": 2.433712121212121,
      "grad_norm": 6.110098838806152,
      "learning_rate": 1.1206896551724138e-05,
      "loss": 2.5881,
      "step": 2570
    },
    {
      "epoch": 2.4431818181818183,
      "grad_norm": 6.45134162902832,
      "learning_rate": 1.1019490254872564e-05,
      "loss": 2.7982,
      "step": 2580
    },
    {
      "epoch": 2.452651515151515,
      "grad_norm": 5.982999324798584,
      "learning_rate": 1.083208395802099e-05,
      "loss": 2.3254,
      "step": 2590
    },
    {
      "epoch": 2.462121212121212,
      "grad_norm": 3.9508378505706787,
      "learning_rate": 1.0644677661169416e-05,
      "loss": 2.7443,
      "step": 2600
    },
    {
      "epoch": 2.471590909090909,
      "grad_norm": 6.034092426300049,
      "learning_rate": 1.0457271364317842e-05,
      "loss": 2.5445,
      "step": 2610
    },
    {
      "epoch": 2.481060606060606,
      "grad_norm": 5.215603351593018,
      "learning_rate": 1.0269865067466268e-05,
      "loss": 2.4737,
      "step": 2620
    },
    {
      "epoch": 2.490530303030303,
      "grad_norm": 5.730741500854492,
      "learning_rate": 1.0082458770614692e-05,
      "loss": 2.8889,
      "step": 2630
    },
    {
      "epoch": 2.5,
      "grad_norm": 6.629499912261963,
      "learning_rate": 9.89505247376312e-06,
      "loss": 2.9835,
      "step": 2640
    },
    {
      "epoch": 2.5094696969696972,
      "grad_norm": 5.8935770988464355,
      "learning_rate": 9.707646176911544e-06,
      "loss": 3.1708,
      "step": 2650
    },
    {
      "epoch": 2.518939393939394,
      "grad_norm": 5.863354206085205,
      "learning_rate": 9.52023988005997e-06,
      "loss": 2.5494,
      "step": 2660
    },
    {
      "epoch": 2.528409090909091,
      "grad_norm": 5.168139457702637,
      "learning_rate": 9.332833583208397e-06,
      "loss": 2.4945,
      "step": 2670
    },
    {
      "epoch": 2.537878787878788,
      "grad_norm": 6.378106117248535,
      "learning_rate": 9.145427286356823e-06,
      "loss": 2.3995,
      "step": 2680
    },
    {
      "epoch": 2.547348484848485,
      "grad_norm": 3.791708469390869,
      "learning_rate": 8.958020989505247e-06,
      "loss": 2.8885,
      "step": 2690
    },
    {
      "epoch": 2.5568181818181817,
      "grad_norm": 6.5189738273620605,
      "learning_rate": 8.770614692653675e-06,
      "loss": 3.0544,
      "step": 2700
    },
    {
      "epoch": 2.566287878787879,
      "grad_norm": 4.136637210845947,
      "learning_rate": 8.583208395802099e-06,
      "loss": 2.5966,
      "step": 2710
    },
    {
      "epoch": 2.5757575757575757,
      "grad_norm": 5.349743843078613,
      "learning_rate": 8.395802098950525e-06,
      "loss": 2.9027,
      "step": 2720
    },
    {
      "epoch": 2.5852272727272725,
      "grad_norm": 5.455042362213135,
      "learning_rate": 8.208395802098951e-06,
      "loss": 2.8066,
      "step": 2730
    },
    {
      "epoch": 2.5946969696969697,
      "grad_norm": 5.3912787437438965,
      "learning_rate": 8.020989505247377e-06,
      "loss": 2.4518,
      "step": 2740
    },
    {
      "epoch": 2.6041666666666665,
      "grad_norm": 5.757827281951904,
      "learning_rate": 7.833583208395801e-06,
      "loss": 3.0423,
      "step": 2750
    },
    {
      "epoch": 2.6136363636363638,
      "grad_norm": 4.654381275177002,
      "learning_rate": 7.646176911544228e-06,
      "loss": 2.775,
      "step": 2760
    },
    {
      "epoch": 2.6231060606060606,
      "grad_norm": 6.395894527435303,
      "learning_rate": 7.4587706146926535e-06,
      "loss": 2.8455,
      "step": 2770
    },
    {
      "epoch": 2.632575757575758,
      "grad_norm": 5.26940393447876,
      "learning_rate": 7.27136431784108e-06,
      "loss": 2.8226,
      "step": 2780
    },
    {
      "epoch": 2.6420454545454546,
      "grad_norm": 5.13749885559082,
      "learning_rate": 7.0839580209895065e-06,
      "loss": 2.7045,
      "step": 2790
    },
    {
      "epoch": 2.6515151515151514,
      "grad_norm": 6.188582420349121,
      "learning_rate": 6.896551724137932e-06,
      "loss": 3.0785,
      "step": 2800
    },
    {
      "epoch": 2.6609848484848486,
      "grad_norm": 5.395571708679199,
      "learning_rate": 6.709145427286358e-06,
      "loss": 2.7078,
      "step": 2810
    },
    {
      "epoch": 2.6704545454545454,
      "grad_norm": 4.964759349822998,
      "learning_rate": 6.521739130434783e-06,
      "loss": 2.9967,
      "step": 2820
    },
    {
      "epoch": 2.679924242424242,
      "grad_norm": 6.158221244812012,
      "learning_rate": 6.334332833583209e-06,
      "loss": 2.9117,
      "step": 2830
    },
    {
      "epoch": 2.6893939393939394,
      "grad_norm": 4.447978496551514,
      "learning_rate": 6.146926536731634e-06,
      "loss": 2.6016,
      "step": 2840
    },
    {
      "epoch": 2.6988636363636362,
      "grad_norm": 4.531870365142822,
      "learning_rate": 5.95952023988006e-06,
      "loss": 2.3581,
      "step": 2850
    },
    {
      "epoch": 2.7083333333333335,
      "grad_norm": 5.613025188446045,
      "learning_rate": 5.772113943028486e-06,
      "loss": 2.4034,
      "step": 2860
    },
    {
      "epoch": 2.7178030303030303,
      "grad_norm": 5.08083963394165,
      "learning_rate": 5.584707646176912e-06,
      "loss": 3.0086,
      "step": 2870
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 5.47483491897583,
      "learning_rate": 5.397301349325338e-06,
      "loss": 2.5979,
      "step": 2880
    },
    {
      "epoch": 2.7367424242424243,
      "grad_norm": 5.742126941680908,
      "learning_rate": 5.2098950524737635e-06,
      "loss": 2.7349,
      "step": 2890
    },
    {
      "epoch": 2.746212121212121,
      "grad_norm": 5.77949857711792,
      "learning_rate": 5.0224887556221895e-06,
      "loss": 2.6639,
      "step": 2900
    },
    {
      "epoch": 2.7556818181818183,
      "grad_norm": 5.449282169342041,
      "learning_rate": 4.835082458770615e-06,
      "loss": 2.9381,
      "step": 2910
    },
    {
      "epoch": 2.765151515151515,
      "grad_norm": 4.639759063720703,
      "learning_rate": 4.647676161919041e-06,
      "loss": 2.815,
      "step": 2920
    },
    {
      "epoch": 2.774621212121212,
      "grad_norm": 6.46406888961792,
      "learning_rate": 4.460269865067467e-06,
      "loss": 2.7645,
      "step": 2930
    },
    {
      "epoch": 2.784090909090909,
      "grad_norm": 5.234225273132324,
      "learning_rate": 4.272863568215892e-06,
      "loss": 2.9509,
      "step": 2940
    },
    {
      "epoch": 2.793560606060606,
      "grad_norm": 6.061874866485596,
      "learning_rate": 4.085457271364318e-06,
      "loss": 2.8241,
      "step": 2950
    },
    {
      "epoch": 2.8030303030303028,
      "grad_norm": 4.95209264755249,
      "learning_rate": 3.898050974512744e-06,
      "loss": 2.4872,
      "step": 2960
    },
    {
      "epoch": 2.8125,
      "grad_norm": 7.161643981933594,
      "learning_rate": 3.7106446776611696e-06,
      "loss": 2.4433,
      "step": 2970
    },
    {
      "epoch": 2.8219696969696972,
      "grad_norm": 6.77622127532959,
      "learning_rate": 3.5232383808095952e-06,
      "loss": 2.5793,
      "step": 2980
    },
    {
      "epoch": 2.831439393939394,
      "grad_norm": 5.091814041137695,
      "learning_rate": 3.335832083958021e-06,
      "loss": 2.7246,
      "step": 2990
    },
    {
      "epoch": 2.840909090909091,
      "grad_norm": 5.520253658294678,
      "learning_rate": 3.1484257871064465e-06,
      "loss": 2.4538,
      "step": 3000
    },
    {
      "epoch": 2.850378787878788,
      "grad_norm": 6.018923282623291,
      "learning_rate": 2.961019490254873e-06,
      "loss": 2.6469,
      "step": 3010
    },
    {
      "epoch": 2.859848484848485,
      "grad_norm": 6.268494606018066,
      "learning_rate": 2.7736131934032985e-06,
      "loss": 3.015,
      "step": 3020
    },
    {
      "epoch": 2.8693181818181817,
      "grad_norm": 5.517399787902832,
      "learning_rate": 2.586206896551724e-06,
      "loss": 2.7535,
      "step": 3030
    },
    {
      "epoch": 2.878787878787879,
      "grad_norm": 5.423760414123535,
      "learning_rate": 2.3988005997001498e-06,
      "loss": 2.7752,
      "step": 3040
    },
    {
      "epoch": 2.8882575757575757,
      "grad_norm": 5.553169250488281,
      "learning_rate": 2.211394302848576e-06,
      "loss": 2.9275,
      "step": 3050
    },
    {
      "epoch": 2.8977272727272725,
      "grad_norm": 5.550352096557617,
      "learning_rate": 2.0239880059970014e-06,
      "loss": 2.618,
      "step": 3060
    },
    {
      "epoch": 2.9071969696969697,
      "grad_norm": 5.152230262756348,
      "learning_rate": 1.8365817091454275e-06,
      "loss": 2.8662,
      "step": 3070
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 5.937163829803467,
      "learning_rate": 1.6491754122938533e-06,
      "loss": 2.7362,
      "step": 3080
    },
    {
      "epoch": 2.9261363636363638,
      "grad_norm": 5.8912506103515625,
      "learning_rate": 1.461769115442279e-06,
      "loss": 2.2995,
      "step": 3090
    },
    {
      "epoch": 2.9356060606060606,
      "grad_norm": 5.431722640991211,
      "learning_rate": 1.2743628185907047e-06,
      "loss": 3.0536,
      "step": 3100
    },
    {
      "epoch": 2.945075757575758,
      "grad_norm": 5.741662502288818,
      "learning_rate": 1.0869565217391306e-06,
      "loss": 2.8022,
      "step": 3110
    },
    {
      "epoch": 2.9545454545454546,
      "grad_norm": 5.751827239990234,
      "learning_rate": 8.995502248875562e-07,
      "loss": 3.0296,
      "step": 3120
    },
    {
      "epoch": 2.9640151515151514,
      "grad_norm": 4.900834083557129,
      "learning_rate": 7.12143928035982e-07,
      "loss": 2.6091,
      "step": 3130
    },
    {
      "epoch": 2.9734848484848486,
      "grad_norm": 4.827839374542236,
      "learning_rate": 5.247376311844078e-07,
      "loss": 2.6421,
      "step": 3140
    },
    {
      "epoch": 2.9829545454545454,
      "grad_norm": 5.509980201721191,
      "learning_rate": 3.3733133433283364e-07,
      "loss": 2.6792,
      "step": 3150
    },
    {
      "epoch": 2.992424242424242,
      "grad_norm": 5.1977925300598145,
      "learning_rate": 1.4992503748125936e-07,
      "loss": 2.9777,
      "step": 3160
    }
  ],
  "logging_steps": 10,
  "max_steps": 3168,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.3666826324017152e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
